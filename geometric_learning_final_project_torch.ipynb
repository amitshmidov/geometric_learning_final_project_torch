{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geometric_learning_final_project_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqsE4ZhF7b3FjkVCEgUtQp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitshmidov/geometric_learning_final_project_torch/blob/main/geometric_learning_final_project_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAfrRgNux7Sd",
        "outputId": "5ac3a6a8-b9de-4a34-fb8e-c66564f0b58c"
      },
      "source": [
        "!pip install trimesh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trimesh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/eb/28cd472f53790ff5ba5d8e9138cc5f8cc0a5f761d1114454ff0d37006b7f/trimesh-3.9.8-py3-none-any.whl (629kB)\n",
            "\r\u001b[K     |▌                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 22.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 27.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 21.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 14.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 143kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 153kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 174kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 184kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 204kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 225kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 245kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 256kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 266kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 286kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 307kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 317kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 327kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 337kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 348kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 358kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 368kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 378kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 389kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 399kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 409kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 419kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 430kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 440kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 450kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 460kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 471kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 481kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 491kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 501kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 512kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 522kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 532kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 542kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 552kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 563kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 573kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 583kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 593kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 604kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 614kB 12.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 624kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trimesh) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh) (53.0.0)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-3.9.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwA3Nf5x-AD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import trimesh\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as grad\n",
        "import itertools\n",
        "\n",
        "from trimesh.sample import sample_surface, sample_surface_even\n",
        "from typing import List"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keYNHDu8y14V"
      },
      "source": [
        "ACTIVATIONS = dict(\n",
        "    relu=nn.ReLU, \n",
        "    lrelu=nn.LeakyReLU\n",
        ")\n",
        "\n",
        "CONVS = dict(\n",
        "    one_dims=nn.Conv1d,\n",
        "    two_dims=nn.Conv2d\n",
        ")\n",
        "\n",
        "BATCHNORMS = dict(\n",
        "    one_dims=nn.BatchNorm1d,\n",
        "    two_dims=nn.BatchNorm2d\n",
        ")\n",
        "\n",
        "\n",
        "def conv_block(\n",
        "        channels: tuple,\n",
        "        dims: bool = False,\n",
        "        activation: str = 'relu',\n",
        "        activation_params = dict()\n",
        ") -> nn.Sequential:\n",
        "    \"\"\"\n",
        "    Return a convolutional Sequential block .\n",
        "    :param channels: channels of each layer.\n",
        "    :param dims: dims=False for 1d operations, True for 2 operations.\n",
        "    :param activation: type of activation.\n",
        "    :param activation_params: specific activation params.\n",
        "    :return: nn.Sequential object.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    mode = 'one_dims' if not dims else 'two_dims'\n",
        "    act = ACTIVATIONS[activation]\n",
        "    conv = CONVS[mode]\n",
        "    bn = BATCHNORMS[mode]\n",
        "    for i in range(len(channels) - 1):\n",
        "        layers.append(conv(in_channels=channels[i], out_channels=channels[i + 1], kernel_size=1))\n",
        "        layers.append(bn(num_features=channels[i + 1]))\n",
        "        layers.append(act(**activation_params))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def linear_block(\n",
        "        features: tuple,\n",
        "        out_features: int,\n",
        "        dropout: float = 0,\n",
        "        activation: str = 'relu',\n",
        "        activation_params = dict()\n",
        ") -> nn.Sequential:\n",
        "    \"\"\"\n",
        "    Return a linear Sequential block .\n",
        "    :param features: features of each layer.\n",
        "    :param out_features: output features of the block.\n",
        "    :param dropout: add dropout layer after each linear block (0 to disable).\n",
        "    :param activation: type of activation.\n",
        "    :param activation_params: specific activation params.\n",
        "    :return: nn.Sequential object.\n",
        "    \"\"\"\n",
        "    assert 0 <= dropout < 1\n",
        "    \n",
        "    act = ACTIVATIONS[activation]\n",
        "    layers = []\n",
        "    for i in range(len(features) - 1):\n",
        "        layers.append(nn.Linear(in_features=features[i], out_features=features[i + 1]))\n",
        "        layers.append(nn.BatchNorm1d(num_features=features[i + 1]))\n",
        "        layers.append(act(**activation_params))\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(p=dropout))\n",
        "    layers.append(nn.Linear(in_features=features[-1], out_features=out_features))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdKWrjPay8b5"
      },
      "source": [
        "def torch_(x):\n",
        "    \"\"\"\n",
        "    Converts torch object to device and dtype.\n",
        "    :param x: any torch object.\n",
        "    :return: x in the current available device and dtype.\n",
        "    \"\"\"\n",
        "    return x.to(torch_configs['dev']).type(torch_configs['type'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSAZrSRByxCi"
      },
      "source": [
        "class TNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_points: int = 2000,\n",
        "            num_features: int = 3,\n",
        "            encoder: tuple = (64, 128, 1024),\n",
        "            decoder: tuple = (1024, 512, 256),\n",
        "            activation: str = 'relu',\n",
        "            activation_params = dict()\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param num_points: number of points of each shape.\n",
        "        :param num_features: number of features of each point.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert encoder[-1] == decoder[0]\n",
        "\n",
        "        self.num_points = num_points\n",
        "        self.dims = num_features\n",
        "\n",
        "        self.identity = grad.Variable(torch_(torch.eye(self.dims, requires_grad=True).view(-1)))\n",
        "\n",
        "        channels = (num_features,) + encoder\n",
        "        self.encoder = conv_block(channels, activation=activation, activation_params=activation_params)\n",
        "\n",
        "        self.decoder = linear_block(features=decoder, out_features=num_features ** 2, dropout=0,\n",
        "                                   activation=activation, activation_params=activation_params)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = F.max_pool1d(x, kernel_size=self.num_points).squeeze(2)\n",
        "        x = self.decoder(x)\n",
        "        x += self.identity\n",
        "        x = x.view(-1, self.dims, self.dims)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo_pcC0IyeEz"
      },
      "source": [
        "class Momenet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_features: int,\n",
        "            out_classes: int,\n",
        "            num_points: int,\n",
        "            lifting: List[nn.Module],\n",
        "            hiddent: int = 12,\n",
        "            hidden1: tuple = (64,),\n",
        "            hidden2: tuple = (64, 64, 128, 1024),\n",
        "            hidden3: tuple = (1024, 512, 256),\n",
        "            activation: str = 'relu',\n",
        "            activation_params = dict()\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param num_features: #features of each point.\n",
        "        :param out_classes: #classes.\n",
        "        :param num_points: #points in each shape.\n",
        "        :param hiddent: the dimension of the feature transform.\n",
        "        :param hidden1: channels of the 1st conv block (called mlp1 in paper).\n",
        "        :param hidden2: channels of the 2nd conv block (called mlp2 in paper).\n",
        "        :param hidden3: features of the classifier (called mlp in paper).\n",
        "        :param activation: type of activation.\n",
        "        :param activation_params: specific activation params.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.spatial_transform = TNet(num_points, num_features, activation=activation, activation_params=activation_params)\n",
        "\n",
        "        self.lifts = lifting\n",
        "\n",
        "        channels = (hiddent,) + hidden1\n",
        "        self.mlp1 = conv_block(channels, dims=True,\n",
        "                               activation=activation, activation_params=activation_params)\n",
        "\n",
        "        channels = (hidden1[-1],) + hidden2\n",
        "        self.mlp2 = conv_block(channels, dims=False,\n",
        "                               activation=activation, activation_params=activation_params)\n",
        "\n",
        "        self.mlp3 = linear_block(features=hidden3, out_features=out_classes, dropout=0.4,\n",
        "                                activation=activation, activation_params=activation_params)\n",
        "\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Feed-forward x in the network.\n",
        "        :param x: A tensor with batch data.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        num_points = x.shape[-1]\n",
        "\n",
        "        t = self.spatial_transform(x)\n",
        "        x = torch.bmm(t, x)\n",
        "\n",
        "        for lift in self.lifts:\n",
        "            with torch.no_grad():\n",
        "                x = lift(x)\n",
        "\n",
        "        x = self.mlp1(x)\n",
        "\n",
        "        x = x.max(dim=3).values\n",
        "\n",
        "        x = self.mlp2(x)\n",
        "\n",
        "        x = F.max_pool1d(x, num_points).squeeze(2)\n",
        "\n",
        "        x = self.mlp3(x)\n",
        "\n",
        "        x = self.logsoftmax(x)\n",
        "\n",
        "        return x, t"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk6heQxhzcuq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}